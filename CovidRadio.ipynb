{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision pandas pillow tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq3oL7uV9d7J",
        "outputId": "953f3458-3205-46b1-eabf-583c7fdb32d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "92xGdHJf9gey"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ieee8023/covid-chestxray-dataset.git\n",
        "data_dir = \"covid-chestxray-dataset/images\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPGUvz2n9htq",
        "outputId": "63ea7bd9-877d-4dff-c1c8-9753e991e9af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'covid-chestxray-dataset'...\n",
            "remote: Enumerating objects: 3641, done.\u001b[K\n",
            "remote: Total 3641 (delta 0), reused 0 (delta 0), pack-reused 3641 (from 1)\u001b[K\n",
            "Receiving objects: 100% (3641/3641), 632.96 MiB | 37.87 MiB/s, done.\n",
            "Resolving deltas: 100% (1450/1450), done.\n",
            "Updating files: 100% (1174/1174), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# create list\n",
        "image_paths = []\n",
        "captions = []\n",
        "\n",
        "for root, _, files in os.walk(data_dir):\n",
        "    for file in files:\n",
        "        if file.endswith(\".png\") or file.endswith(\".jpg\"):\n",
        "            image_paths.append(os.path.join(root, file))\n",
        "            captions.append(os.path.splitext(file)[0])  # کپشن = نام فایل بدون پسوند\n",
        "\n",
        "# first 50 pic for test\n",
        "image_paths = image_paths[:50]\n",
        "captions = captions[:50]\n",
        "\n",
        "# ==============================\n",
        "# create vocab\n",
        "# ==============================\n",
        "all_words = set()\n",
        "for cap in captions:\n",
        "    for w in cap.lower().split():\n",
        "        all_words.add(w)\n",
        "\n",
        "vocab = {\"<pad>\":0, \"<start>\":1, \"<end>\":2, \"<unk>\":3}\n",
        "for i, w in enumerate(all_words, start=4):\n",
        "    vocab[w] = i\n",
        "inv_vocab = {v:k for k,v in vocab.items()}\n",
        "\n",
        "# ==============================\n",
        "#  Dataset Class\n",
        "# ==============================\n",
        "class CovidXrayDataset(Dataset):\n",
        "    def __init__(self, image_paths, captions, transform=None, vocab=None):\n",
        "        self.image_paths = image_paths\n",
        "        self.captions = captions\n",
        "        self.transform = transform\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        caption = self.captions[idx]\n",
        "        caption_idx = [self.vocab.get(\"<start>\")]\n",
        "        for word in caption.lower().split():\n",
        "            caption_idx.append(self.vocab.get(word, self.vocab[\"<unk>\"]))\n",
        "        caption_idx.append(self.vocab.get(\"<end>\"))\n",
        "        return img, torch.tensor(caption_idx)\n",
        "\n",
        "# ==============================\n",
        "# ۵️⃣ Transform و DataLoader\n",
        "# ==============================\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=0)\n",
        "    lengths = [len(c)-1 for c in captions]  # طول بدون <start>\n",
        "    return images, captions, lengths\n",
        "\n",
        "dataset = CovidXrayDataset(image_paths, captions, transform, vocab)\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# ==============================\n",
        "# model Encoder + Decoder\n",
        "# ==============================\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        for param in resnet.parameters():\n",
        "            param.requires_grad = False\n",
        "        self.resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "\n",
        "    def forward(self, images):\n",
        "        features = self.resnet(images)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.linear(features)\n",
        "        return features\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        embeddings = self.embed(captions[:,:-1])\n",
        "        inputs = torch.cat((features.unsqueeze(1), embeddings),1)\n",
        "        hiddens, _ = self.lstm(inputs)\n",
        "        outputs = self.linear(hiddens)\n",
        "        return outputs\n",
        "\n",
        "    def sample(self, features, max_len=20):\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        states = None\n",
        "        for _ in range(max_len):\n",
        "            hiddens, states = self.lstm(inputs, states)\n",
        "            outputs = self.linear(hiddens.squeeze(1))\n",
        "            predicted = outputs.argmax(1)\n",
        "            sampled_ids.append(predicted.item())\n",
        "            inputs = self.embed(predicted).unsqueeze(1)\n",
        "        return sampled_ids\n",
        "\n",
        "# ==============================\n",
        "# create model\n",
        "# ==============================\n",
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "encoder = EncoderCNN(embed_size)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
        "\n",
        "# ==============================\n",
        "# train with pack_padded_sequence\n",
        "# ==============================\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = torch.optim.Adam(list(decoder.parameters()) + list(encoder.linear.parameters()), lr=1e-3)\n",
        "\n",
        "for epoch in range(2):\n",
        "    for images, captions, lengths in loader:\n",
        "        outputs = decoder(encoder(images), captions)\n",
        "\n",
        "        packed_outputs = pack_padded_sequence(outputs, lengths, batch_first=True, enforce_sorted=False)\n",
        "        packed_targets = pack_padded_sequence(captions[:,1:], lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        loss = criterion(packed_outputs.data, packed_targets.data)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} done, Loss={loss.item():.4f}\")\n",
        "\n",
        "# ==============================\n",
        "# test\n",
        "# ==============================\n",
        "test_img, _ = dataset[0]  # only 2 sample return\n",
        "with torch.no_grad():\n",
        "    feature = encoder(test_img.unsqueeze(0))\n",
        "    sampled_ids = decoder.sample(feature)\n",
        "\n",
        "sampled_caption = []\n",
        "for idx in sampled_ids:\n",
        "    word = inv_vocab.get(idx, \"<unk>\")\n",
        "    if word==\"<end>\":\n",
        "        break\n",
        "    sampled_caption.append(word)\n",
        "print(\"Generated Caption:\", \" \".join(sampled_caption))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RtB8K-4-saZ",
        "outputId": "4eb0fd77-d9e5-40e5-e5e6-d389b7798e5a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 done, Loss=2.1619\n",
            "Epoch 2 done, Loss=2.0084\n",
            "Generated Caption: 16689_1_6\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}